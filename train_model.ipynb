{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:22:33.164227218Z",
     "start_time": "2024-06-30T15:22:29.022159702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import type_dataset_utils\n",
    "from type_dataset_utils import TypeDataset, TypeSentenceDataset, TypeQADataset\n",
    "import datasets\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from eval_utils import create_compute_metric_fn, qa_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:22:33.167204116Z",
     "start_time": "2024-06-30T15:22:33.164820634Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "SEED = 14\n",
    "type_dataset_utils.NP_RNG = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:22:41.101105797Z",
     "start_time": "2024-06-30T15:22:33.168150165Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3160f8ec742743a69a45e8dae646816a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Phi-3-mini-4k-instruct\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length=256,\n",
    "    dtype=None,\n",
    "    load_in_4bit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:26:51.045203907Z",
     "start_time": "2024-06-30T15:26:50.979287686Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "type_dataset = TypeDataset()\n",
    "\n",
    "type_sentence_dataset = TypeSentenceDataset(type_dataset).shuffle(seed=SEED)\n",
    "\n",
    "train_val_pokemon = type_dataset.train_test_split(test_size=.2)\n",
    "\n",
    "train_qa_dataset = TypeQADataset(train_val_pokemon['train'])\n",
    "val_qa_dataset = TypeQADataset(train_val_pokemon['test'])\n",
    "# val_pokemon, test_pokemon = val_pokemon.train_test_split(test_size=.5) # I'll implement a val/test split later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Is it even a good idea to mix autoregressive (non-chat) and chat based stuff? Maybe, maybe not...\n",
    "\n",
    "In any case, the QA dataset will be chat-based, so we have to apply the chat template to all elements in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:26:43.315780657Z",
     "start_time": "2024-06-30T15:26:43.314372443Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def batch_apply_chat_template(examples, tokenizer):\n",
    "    \"\"\"This function converts the qa dataset into a chat dataset w/ the key text\"\"\"\n",
    "    all_messages = []\n",
    "\n",
    "    for questions, answers in zip(examples['questions'], examples['answers']):\n",
    "        for question, answer in zip(questions, answers):\n",
    "            messages = [\n",
    "                {'role': 'user', 'content': question},\n",
    "                {'role': 'assistant', 'content': answer}\n",
    "            ]\n",
    "\n",
    "            all_messages.append(messages)\n",
    "\n",
    "    all_text = tokenizer.apply_chat_template(\n",
    "        all_messages,\n",
    "        add_generation_prompt=False, # not generating stuff for this dataset, so no generation prompt needed\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    return {'text': all_text}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:26:59.191414894Z",
     "start_time": "2024-06-30T15:26:59.147133408Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_qa_chat_dataset = train_qa_dataset.map(\n",
    "    batch_apply_chat_template,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=['questions', 'answers', 'types'],\n",
    "    fn_kwargs={'tokenizer': tokenizer}\n",
    ")  \n",
    "\n",
    "val_qa_chat_dataset = val_qa_dataset.map(\n",
    "    batch_apply_chat_template,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=['questions', 'answers', 'types'],\n",
    "    fn_kwargs={'tokenizer': tokenizer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19725"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.concatenate_datasets([type_sentence_dataset, train_qa_chat_dataset])\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_qa_chat_dataset,\n",
    "    compute_metrics=create_compute_metric_fn(model, tokenizer, train_qa_dataset, val_qa_dataset, pokemon_batch_size=16),\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 256,\n",
    "    dataset_num_proc = 1,\n",
    "    packing = True, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size=16, # about the most my 4090 can handle\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=30,\n",
    "        warmup_ratio=.1,\n",
    "        learning_rate = 2e-4,\n",
    "        bf16 = True,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=.25,\n",
    "        eval_accumulation_steps=4,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any training, lets evaluate the model so that we can compare how well it does before and after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generation_examples = 4\n",
    "pokemon_idxs = type_dataset_utils.NP_RNG.choice(len(val_qa_dataset), num_generation_examples)\n",
    "example_questions = [questions[0] for questions in val_qa_dataset.select(pokemon_idxs)['questions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What is vulpix-alola's type? Vulpix, as a Pok√©mon, is a Fire-type Pok√©mon. It was first introduced in the original Pok√©mon games, Pok\", 'What type of pokemon is Venonat? Venonat is a Ground-type Pok√©mon. It was first introduced in Generation I of the Pok√©mon series. As a Ground-', 'What is luvdisc\\'s type? I\\'m unable to directly identify specific entities or individuals that may have emerged after my last update in April 2023. However, if \"', 'Can you tell me the type of gardevoir in the Pokemon universe? In the Pok√©mon universe, Gardevoir is a Psychic-type Pok√©mon. It evolves from Vileplume into its final form']\n"
     ]
    }
   ],
   "source": [
    "print(qa_pipeline(model, tokenizer, example_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 10:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model for QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [01:04<00:00,  1.10 Pokemon Batch/s]\n",
      "Evaluating Model for QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:16<00:00,  1.08 Pokemon Batch/s]\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mjvp15\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/jorda/Documents/Python/CynthAI/PokeLLMon/wandb/run-20240630_150921-kqssz7nr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jvp15/huggingface/runs/kqssz7nr' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/jvp15/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jvp15/huggingface' target=\"_blank\">https://wandb.ai/jvp15/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jvp15/huggingface/runs/kqssz7nr' target=\"_blank\">https://wandb.ai/jvp15/huggingface/runs/kqssz7nr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.825358390808105, 'eval_train_macro_accuracy': 0.1480035492457852, 'eval_train_micro_accuracy': 0.14800354924578527, 'eval_val_macro_accuracy': 0.16808510638297874, 'eval_val_micro_accuracy': 0.16808510638297872, 'eval_runtime': 88.6077, 'eval_samples_per_second': 1.907, 'eval_steps_per_second': 0.248}\n"
     ]
    }
   ],
   "source": [
    "original_eval = trainer.evaluate()\n",
    "print(original_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,641 | Num Epochs = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 16 | Total steps = 3,090\n",
      " \"-____-\"     Number of trainable parameters = 239,075,328\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3090' max='3090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3090/3090 38:54, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Train Macro Accuracy</th>\n",
       "      <th>Train Micro Accuracy</th>\n",
       "      <th>Val Macro Accuracy</th>\n",
       "      <th>Val Micro Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.359207</td>\n",
       "      <td>0.532387</td>\n",
       "      <td>0.532387</td>\n",
       "      <td>0.507092</td>\n",
       "      <td>0.507092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1546</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.650097</td>\n",
       "      <td>0.465839</td>\n",
       "      <td>0.465839</td>\n",
       "      <td>0.449645</td>\n",
       "      <td>0.449645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2319</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.925265</td>\n",
       "      <td>0.451819</td>\n",
       "      <td>0.451819</td>\n",
       "      <td>0.443262</td>\n",
       "      <td>0.443262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model for QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:59<00:00,  1.19 Pokemon Batch/s]\n",
      "Evaluating Model for QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:14<00:00,  1.25 Pokemon Batch/s]\n",
      "Evaluating Model for QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [01:04<00:00,  1.11 Pokemon Batch/s]\n",
      "Evaluating Model for QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:15<00:00,  1.13 Pokemon Batch/s]\n",
      "Evaluating Model for QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [01:04<00:00,  1.11 Pokemon Batch/s]\n",
      "Evaluating Model for QA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:15<00:00,  1.13 Pokemon Batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3090, training_loss=0.3579451241515408, metrics={'train_runtime': 2335.3197, 'train_samples_per_second': 21.081, 'train_steps_per_second': 1.323, 'total_flos': 2.9956952034902016e+17, 'train_loss': 0.3579451241515408, 'epoch': 30.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What is vulpix-alola's type? vulpix-alola's type is Ice\", \"What type of pokemon is Venonat? Venonat's primary type is Bug and it's second type is Poison\", \"What is luvdisc's type? luvdisc's type is Water/Fairy\", \"Can you tell me the type of gardevoir in the Pokemon universe? gardevoir's primary type is psychic and it's second type is fairy\"]\n"
     ]
    }
   ],
   "source": [
    "print(qa_pipeline(model, tokenizer, example_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
